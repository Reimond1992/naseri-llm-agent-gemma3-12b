# LLM Agent Gemma3:12B â€“ Full Quick Start (FastAPI)

This guide shows how to run the **LLM Agent API** locally using Python and FastAPI.  
The agent uses **Ollama Gemma3:12B** to answer user messages.  

> No Docker is needed â€“ just Python and Ollama.

---

## ðŸ”¹ Prerequisites

- **Python 3.12** installed  
  [Python Downloads](https://www.python.org/downloads/)
- **Ollama LLM** installed  
  Make sure **Gemma3:12B** model is downloaded.
- Ollama API must be running on:  
  `http://localhost:11434/api/generate`
- Internet connection to install Python packages

---

## ðŸ”¹ 1. Clone the Repository

```bash
git clone <YOUR_GITHUB_REPO_URL>
cd <repo-folder>
ðŸ”¹ 2. Set Up Python Environment
Create a virtual environment:

python -m venv venv
Activate it:

Windows:

venv\Scripts\activate
Linux / Mac:

source venv/bin/activate
ðŸ”¹ 3. Install Dependencies
Make sure you have requirements.txt in the project root, then run:

pip install -r requirements.txt
This will install:

fastapi â€“ the web framework

uvicorn â€“ the server

httpx â€“ for calling Ollama API

pydantic & pydantic-settings â€“ for configuration

ðŸ”¹ 4. Configure Environment Variables
Create a .env file in the project root:

LLM_URL=http://localhost:11434/api/generate
DATABASE_URL=sqlite:///chat.db
TIMEOUT=30
DATABASE_URL points to the SQLite database that stores chat logs.

ðŸ”¹ 5. Run the API
Start the FastAPI server:

uvicorn main:app --host 0.0.0.0 --port 8000 --reload
The API will be available at: http://localhost:8000

Swagger UI for testing: http://localhost:8000/docs

ðŸ”¹ 6. Test the /chat Endpoint
Send a POST request:

POST http://localhost:8000/chat
Content-Type: application/json

{
  "message": "Hello, how are you?"
}
Example JSON response:

{
  "response": "Ø³Ù„Ø§Ù…! Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ±Ù‡ØŸ"
}
ðŸ”¹ 7. Database
SQLite database file: chat.db (created automatically in project root)

Fields stored per message:

message, response, timestamp, user_id, session_id, intent, response_length, language, processing_time, tool_used

ðŸ”¹ 8. Project Structure
llm-agent-gemma3/
â”œâ”€â”€ main.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.db
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ollama.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.py
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
ðŸ”¹ 9. How It Works
User sends a message to /chat

OllamaAgent analyzes the message (pre-processing)

Sends the processed prompt to Ollama Gemma3:12B

Receives a response and stores both message & response in chat.db

Returns the response to the user

Designed to be easily extendable with Tools, Memory, or multiple LLMs.

ðŸ”¹ 10. Optional Notes
You can change port or host in uvicorn command

Logs are stored in SQLite; you can switch to PostgreSQL if needed

API can be tested using Swagger, Postman, or any HTTP client

Make sure Ollama is running before starting FastAPI

ðŸ”¹ 11. Recommended Usage
Always use a virtual environment for a clean setup

For repeated usage, you can run FastAPI in the background:

uvicorn main:app --host 0.0.0.0 --port 8000
